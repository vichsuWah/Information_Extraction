{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import post_process\n",
    "\n",
    "text = '（ＦＡＸ） ０３－６７５８－８０６４'\n",
    "value = '〒１０5-０００１東京都港区虎'\n",
    "\n",
    "post_process(value, 'a', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, glob, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import unicodedata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Info] Load Cannon_Dataset complete !! len:9850\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_tags(cinnamon_path):\n",
    "    tags = set()\n",
    "    files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "    for file in files:\n",
    "        dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "        label_str = filter(lambda i:(type(i) is str), dataframe['Tag'])\n",
    "        def split(strings):\n",
    "            out = list()\n",
    "            for string in strings: \n",
    "                out += string.split(\";\")\n",
    "            out = [unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag)) for tag in out]\n",
    "            return out\n",
    "        items = split(label_str)\n",
    "        tags.update(items)\n",
    "    return tuple(sorted(list(tags)))\n",
    "\n",
    "tags = ('仕様書交付期限',\n",
    "         '入札件名',\n",
    "         '入札書締切日時',\n",
    "         '入札書送付先',\n",
    "         '入札書送付先部署/担当者名',\n",
    "         '公告日',\n",
    "         '施設名',\n",
    "         '調達年度',\n",
    "         '調達終了日',\n",
    "         '調達開始日',\n",
    "         '資格申請締切日時',\n",
    "         '資格申請送付先',\n",
    "         '資格申請送付先部署/担当者名',\n",
    "         '質問票締切日時',\n",
    "         '質問箇所TEL/FAX',\n",
    "         '質問箇所所属/担当者',\n",
    "         '都道府県',\n",
    "         '開札場所',\n",
    "         '開札日時',\n",
    "         '需要場所(住所)')\n",
    "\n",
    "def clean_str(s):\n",
    "    return str(s).replace('イ．','').replace('ア．','').replace('．','').replace(' ','')\n",
    "\n",
    "def sub_idx_finder(list1, list2, t=None):            \n",
    "            if t=='入札件名':\n",
    "                for i in range(len(list1)-len(list2)+1):\n",
    "                    find = True\n",
    "                    hit, miss = 0, 0\n",
    "                    for j in range(len(list2)):\n",
    "                        if list1[i+j] != list2[j]: \n",
    "                            find = False\n",
    "                            miss += 1\n",
    "                        else:\n",
    "                            hit += 1\n",
    "                    if miss < len(list2)/4:\n",
    "                        find = True\n",
    "                    if find:\n",
    "                        return i\n",
    "            elif t=='需要場所(住所)': #反過來找\n",
    "                for i in range(len(list1)-len(list2), -1, -1):\n",
    "                    find = True\n",
    "                    hit, miss = 0, 0\n",
    "                    for j in range(len(list2)):\n",
    "                        if list1[i+j] != list2[j]: \n",
    "                            find = False\n",
    "                            miss += 1\n",
    "                        else:\n",
    "                            hit += 1\n",
    "                    if miss < len(list2)/6:\n",
    "                        find = True\n",
    "                    if find:\n",
    "                        return i          \n",
    "            elif t=='質問箇所所属/担当者': #反過來找\n",
    "                for i in range(len(list1)-len(list2), -1, -1):\n",
    "                    find = True\n",
    "                    hit, miss = 0, 0\n",
    "                    for j in range(len(list2)):\n",
    "                        if list1[i+j] != list2[j]: \n",
    "                            find = False\n",
    "                            miss += 1\n",
    "                        else:\n",
    "                            hit += 1\n",
    "                    if miss < len(list2)/4:\n",
    "                        find = True\n",
    "                    if find:\n",
    "                        return i           \n",
    "            elif t=='質問箇所TEL/FAX': #反過來找\n",
    "                for i in range(len(list1)-len(list2), -1, -1):\n",
    "                    find = True\n",
    "                    hit, miss = 0, 0\n",
    "                    for j in range(len(list2)):\n",
    "                        if list1[i+j] != list2[j]: \n",
    "                            find = False\n",
    "                            miss += 1\n",
    "                        else:\n",
    "                            hit += 1\n",
    "                    if miss < len(list2)/3:\n",
    "                        find = True\n",
    "                    if find:\n",
    "                        return i    \n",
    "            else: #正向找\n",
    "                for i in range(len(list1)-len(list2)+1):\n",
    "                    find = True\n",
    "                    hit, miss = 0, 0\n",
    "                    for j in range(len(list2)):\n",
    "                        if list1[i+j] != list2[j]: \n",
    "                            find = False\n",
    "                            miss += 1\n",
    "                        else:\n",
    "                            hit += 1\n",
    "                    if find:\n",
    "                        return i                \n",
    "            return None\n",
    "\n",
    "class Cinnamon_Dataset_v2(Dataset):\n",
    "    def __init__(self, cinnamon_path, tokenizer, tags=None):        \n",
    "        def get_samples(cinnamon_path):\n",
    "            datas = []\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                doc_id = file[file.find('ca_data/')+8:file.find('.pdf.xlsx')]\n",
    "                \n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                dataframe['Parent Index'] = dataframe['Parent Index'].fillna(1)\n",
    "                \n",
    "                for item in dataframe.iterrows(): \n",
    "                    #item:(Page No, Text, Index, Parent Index, Is Title, Is Table, Tag, Value)\n",
    "                    item = item[1]\n",
    "                    \n",
    "                    doc, index = doc_id, item['Index']\n",
    "                    text, p_text = item['Text'], dataframe.loc[dataframe['Index']==item['Parent Index'],'Text'].item()\n",
    "                    tags, values = item['Tag'], item['Value']\n",
    "                    \n",
    "                    datas.append({'doc':doc_id,'index':index,\n",
    "                                  'text':text, 'p_text':p_text,\n",
    "                                  'tags':tags,'values':values})\n",
    "                #print(datas)\n",
    "            return datas\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = get_samples(cinnamon_path)\n",
    "        self.tags = get_tags(cinnamon_path) if tags is None else tags\n",
    "\n",
    "        print(f'\\t[Info] Load Cannon_Dataset_v2 complete !! len:{self.__len__()}')    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "            \n",
    "    def collate_fn(self, samples):        \n",
    "        tokenizer, TAGS = self.tokenizer, self.tags\n",
    "            \n",
    "        CLS, SEP, PAD = tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id\n",
    "                \n",
    "        ## text tokenized, label vectoized\n",
    "        b_ids, b_labels, b_masks = [], [], []\n",
    "        for sample in samples:            \n",
    "            text, p_text, tags, values = sample['text'],sample['p_text'],sample['tags'],sample['values']\n",
    "            \n",
    "            # string cleaning\n",
    "            text = clean_str(text)\n",
    "            p_text = clean_str(p_text)\n",
    "            tags = unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tags)) if tags is not np.nan else tags\n",
    "            values = clean_str(values)\n",
    "                    \n",
    "            # text to tokens\n",
    "            text_ids = tokenizer.encode(text)[1:-1]\n",
    "            p_text_ids = tokenizer.encode(p_text)[1:-1]\n",
    "                \n",
    "            # input, output, mask\n",
    "            ids = [CLS] + text_ids + [SEP] + p_text_ids + [SEP]\n",
    "            labels = [[0 for i in range(len(TAGS))] for j in range(len(ids)) ]\n",
    "            masks = [0] + [1 for i in range(len(text_ids))] + [0 for i in range(len(ids)-len(text_ids)-1)]\n",
    "                                \n",
    "            # assign label \n",
    "            if isinstance(tags, str):\n",
    "                for tag,value in zip(tags.split(';'), str(values).split(';')):   \n",
    "                    tag = unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag))\n",
    "                        \n",
    "                    value_ids = tokenizer.encode(value)[1:-1]\n",
    "                    pivote = sub_idx_finder(text_ids, value_ids, tag)                        \n",
    "                    if pivote is not None:\n",
    "                        for k in range(len(value_ids)):\n",
    "                            labels[1+pivote+k][TAGS.index(tag)] = 1\n",
    "                    else:\n",
    "                        print(\"\\t[ERROR] pivote not found \")\n",
    "            b_ids.append(ids)\n",
    "            b_labels.append(labels)\n",
    "            b_masks.append(masks)\n",
    "\n",
    "        ## pad to same lenght\n",
    "        max_len = min([max([len(s) for s in b_ids]), 512])\n",
    "        for i,(ids, labels, masks) in enumerate(zip(b_ids, b_labels, b_masks)):            \n",
    "            ids = ids[:max_len]\n",
    "            ids += [PAD]*(max_len-len(ids))\n",
    "            b_ids[i] = ids\n",
    "            \n",
    "            labels = labels[:max_len]\n",
    "            labels += [[0 for j in range(len(TAGS))] for k in range(max_len-len(labels))]\n",
    "            b_labels[i] = labels\n",
    "            \n",
    "            masks = masks[:max_len]\n",
    "            masks += [0]*(max_len-len(masks))\n",
    "            b_masks[i] = masks\n",
    "\n",
    "        return torch.tensor(b_ids), torch.tensor(b_labels), torch.tensor(b_masks)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    2,  1266,  1759,  1711,  5580,  5107,  2988,    35,  3663, 10582,\n",
       "           4803,  2650,     3, 17175,   170, 29121,     3,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    2,  1097,    11,    34,   344,     7,     9,     6,  5686,  1711,\n",
       "            118,     5,   323,  7460,     5,  1768,     6,  5686,  1711,    13,\n",
       "              5,   284,     5,  3279,   459,     5,  1768,     7,  2367,     3,\n",
       "           4478,  1097,     5,  4980,     7, 16745, 24169,  7536,     3],\n",
       "         [    2,     5,   104,     5,   859,  5292,     5,  3225,    11,  1750,\n",
       "             16, 17175,    15,    10,   104,    11,  1042,     3,    23,   101,\n",
       "             24, 20781,   104,     5,  1067,  1559,  3688,    26,    20,    10,\n",
       "           3571, 10240,    11,  1770,     3,     0,     0,     0,     0],\n",
       "         [    2,  3462,     5,   859,     6,    36,  6899, 29034,    38,   320,\n",
       "             36,   192,    38,  5311,    36,   277,    38,     5, 11599,     7,\n",
       "          18668,    26,    20,    10,   104,    12,     3,    23,    25,    24,\n",
       "            288,     5,  1977,    11,  1520,  7018,    45,     8,     3]]),\n",
       " tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]],\n",
       " \n",
       "         [[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          ...,\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]]),\n",
       " tensor([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[100]\n",
    "\n",
    "        \n",
    "abc = 'イ．．イ．dfイア．ア．afwq  er．adイ．'\n",
    "re.sub(r'アイ． ','',abc)\n",
    "abc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from dataset import DataLoader\n",
    "from train import BertTokenizer, BertJapaneseTokenizer, Model, pretrained_weights, train\n",
    "from main import parse_args\n",
    "\n",
    "args = parse_args('')\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "\n",
    "train_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/train/', tokenizer,tags)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size = 4,\n",
    "                        num_workers = 1,\n",
    "                        collate_fn=train_dataset.collate_fn,\n",
    "                        shuffle=False)\n",
    "\n",
    "train_dataset.collate_fn([train_dataset[0],train_dataset[1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2,2,1,3],[2,2,2,2,1]])\n",
    "#torch.masked_select(a, torch.tensor([[0],[1]]).expand(2,5))\n",
    "c,d = a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>c</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "      <td>ddd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "      <td>d1111dd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1111</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a     c       11\n",
       "0  1  1111      ddd\n",
       "1  1  1111  d1111dd\n",
       "2  1  1111       33"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['a','c','11'])\n",
    "df = df.append({'a':1,'c':1111,'11':'ddd'}, ignore_index=True)\n",
    "df = df.append({'a':1,'c':1111,'11':'d1111dd'}, ignore_index=True)\n",
    "df = df.append({'a':1,'c':1111,'11':'33'}, ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cinnamon_Dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-abd6c4124282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertJapaneseTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#, do_lower_case=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCinnamon_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/D/ADL2020-SPRING/project/cinnamon/train/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mvalid_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCinnamon_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/media/D/ADL2020-SPRING/project/cinnamon/dev/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m train_dataloader = DataLoader(train_dataset,\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Cinnamon_Dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from dataset import DataLoader\n",
    "from train import BertTokenizer, BertJapaneseTokenizer, Model, pretrained_weights, train\n",
    "from main import parse_args\n",
    "\n",
    "\n",
    "args = parse_args('')\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_weights)#, do_lower_case=True)\n",
    "\n",
    "train_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/train/', tokenizer,tags)\n",
    "valid_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/dev/', tokenizer,tags)\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size = 4,\n",
    "                        num_workers = 8,\n",
    "                        collate_fn=train_dataset.collate_fn,\n",
    "                        shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                             batch_size = 4,\n",
    "                             num_workers = 8,\n",
    "                             collate_fn = valid_dataset.collate_fn,\n",
    "                             shuffle = False)\n",
    "    \n",
    "## train\n",
    "train(args, train_dataloader, valid_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json, glob, torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import re\n",
    "import unicodedata\n",
    "\n",
    "########################################################\n",
    "##################  Cinnamon Dataset  ##################\n",
    "class Cinnamon_Dataset_Testing(Dataset):\n",
    "    def __init__(self, cinnamon_path, tokenizer):\n",
    "        def get_tags(cinnamon_path):\n",
    "            tags = set()\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                label_str = filter(lambda i:(type(i) is str), dataframe['Tag'])\n",
    "                def split(strings):\n",
    "                    out = list()\n",
    "                    for string in strings: \n",
    "                        out += string.split(\";\")\n",
    "                    out = [unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag)) for tag in out]\n",
    "                    return out\n",
    "                items = split(label_str)\n",
    "                tags.update(items)\n",
    "            return tuple(sorted(list(tags)))\n",
    "        \n",
    "        def get_samples(cinnamon_path):\n",
    "            groups = []\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                doc_id = file[file.find('ca_data/')+8:file.find('.pdf.xlsx')]\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                for i in range(10):\n",
    "                    if not isinstance(dataframe['Parent Index'][i], int):\n",
    "                        dataframe['Parent Index'][i] = 0 # index是nan的補 0\n",
    "                dataframe['ID'] = dataframe['Index'].apply(lambda x: \"{}-{}\".format(doc_id,x))\n",
    "                dataframe['id'] = int(doc_id)\n",
    "                \n",
    "                p_index = dataframe.groupby('Parent Index')\n",
    "                for g in list(p_index.groups.keys()):\n",
    "                    groups.append({'doc_id':doc_id,'sample':p_index.get_group(g)})\n",
    "            return groups\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = get_samples(cinnamon_path)\n",
    "        self.tags = get_tags(cinnamon_path)\n",
    "\n",
    "        print(f'\\t[Info] Load Cannon_Dataset complete !! len:{self.__len__()}')    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "            \n",
    "    def collate_fn(self, samples):        \n",
    "        tokenizer, tags = self.tokenizer, self.tags\n",
    "            \n",
    "        CLS, SEP, PAD = tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id\n",
    "        \n",
    "        def zero_vec(): \n",
    "            return [0]*len(tags)\n",
    "        \n",
    "        def sub_idx_finder(list1, list2):            \n",
    "            for i in range(len(list1)-len(list2)):\n",
    "                find = True\n",
    "                hit, miss = 0, 0\n",
    "                for j in range(len(list2)):\n",
    "                    if list1[i+j] != list2[j]: \n",
    "                        find = False\n",
    "                        miss += 1\n",
    "                    else:\n",
    "                        hit += 1\n",
    "                if miss < len(list2)/5:\n",
    "                    find = True\n",
    "                if find:\n",
    "                    return i\n",
    "            #print('yeh')\n",
    "        \n",
    "        ## text tokenized, label vectoized\n",
    "        b_doc_id, b_token_ids, b_output, b_token_indexs = [], [], [], []\n",
    "        for sample in samples:\n",
    "            doc_id = sample['doc_id']\n",
    "            sample = sample['sample']\n",
    "            \n",
    "            token_ids = [CLS]\n",
    "            output = [zero_vec()]\n",
    "            token_indexs = [-1]\n",
    "            for index, text, tag, value in zip(sample['Index'],sample['Text'],sample['Tag'],sample['Value']):\n",
    "                # 全形半形問題\n",
    "                text = str(unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', text)))\n",
    "                #tag = str(unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag))) if tag is not np.nan else tag\n",
    "                #value = str(unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', value))) if value is not np.nan else value\n",
    "                    \n",
    "                ###\n",
    "                ids = tokenizer.encode(text)[1:-1] + [SEP]\n",
    "                labels = [zero_vec()]*(len(ids)-1) + [zero_vec()]\n",
    "                indexs = [index]*(len(ids)-1) + [-1]\n",
    "                '''\n",
    "                if isinstance(tag, str):\n",
    "                    for t,v in zip(tag.split(';'), str(value).split(';')):\n",
    "                        t = unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', t))\n",
    "                        v = unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', v))\n",
    "                        \n",
    "                        ids_v = tokenizer.encode(v)[1:-1]\n",
    "                        pivote = sub_idx_finder(ids, ids_v)\n",
    "                        for k in range(len(ids_v)):\n",
    "                            if pivote is not None:\n",
    "                                labels[pivote+k][tags.index(t)] = 1\n",
    "                '''\n",
    "                token_ids += ids\n",
    "                output += labels\n",
    "                token_indexs += indexs\n",
    "            b_doc_id.append(doc_id)\n",
    "            b_token_ids.append(token_ids)\n",
    "            b_output.append(0)\n",
    "            b_token_indexs.append(token_indexs)\n",
    "\n",
    "        ## pad to same lenght\n",
    "        max_len = min([max([len(s) for s in b_token_ids]), 512])\n",
    "        for idx,(token_ids, output, token_indexs) in enumerate(zip(b_token_ids, b_output, b_token_indexs)):            \n",
    "            token_ids = token_ids[:max_len]\n",
    "            token_ids += [PAD]*(max_len-len(token_ids))\n",
    "            b_token_ids[idx] = token_ids\n",
    "            '''\n",
    "            output = output[:max_len]\n",
    "            output += [zero_vec()]*(max_len-len(output))\n",
    "            b_output[idx] = output\n",
    "            '''\n",
    "            token_indexs = token_indexs[:max_len]\n",
    "            token_indexs += [-1]*(max_len-len(token_indexs))\n",
    "            b_token_indexs[idx] = token_indexs\n",
    "\n",
    "        return torch.tensor(b_token_ids), None, b_token_indexs, b_doc_id, sample\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valid_dataset = Cinnamon_Dataset_Testing('/media/D/ADL2020-SPRING/project/cinnamon/dev/', tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _code(_input):\n",
    "    def abcd(string):\n",
    "        coding = []\n",
    "        for char in string:\n",
    "            status = unicodedata.east_asian_width(char)\n",
    "            if status == 'F':\n",
    "                #print('{0} is full-width.'.format(char))\n",
    "                coding.append('F')\n",
    "            elif status == 'H':\n",
    "                #print('{0} is half-width.'.format(char))\n",
    "                coding.append('H')\n",
    "            else:\n",
    "                #print('{0} is char'.format(char))\n",
    "                coding.append('C')\n",
    "        return coding\n",
    "    if isinstance(_input, list):\n",
    "        string_ls = _input\n",
    "        ddd = []\n",
    "        for s in string_ls:\n",
    "            ddd.append(abcd(s))\n",
    "        return ddd\n",
    "    else:\n",
    "        return abcd(_input)\n",
    "        \n",
    "\n",
    "class Cinnamon_Dataset_Testing(Dataset):\n",
    "    def __init__(self, cinnamon_path, tokenizer, tags=None):\n",
    "        def get_tags(cinnamon_path):\n",
    "            tags = set()\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                label_str = filter(lambda i:(type(i) is str), dataframe['Tag'])\n",
    "                def split(strings):\n",
    "                    out = list()\n",
    "                    for string in strings: \n",
    "                        out += string.split(\";\")\n",
    "                    out = [unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag)) for tag in out]\n",
    "                    return out\n",
    "                items = split(label_str)\n",
    "                tags.update(items)\n",
    "            return tuple(sorted(list(tags)))\n",
    "        \n",
    "        def get_samples(cinnamon_path):\n",
    "            groups = []\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                doc_id = file[file.find('ca_data/')+8:file.find('.pdf.xlsx')]\n",
    "                \n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                dataframe['doc_id'] = [doc_id]*len(dataframe)\n",
    "                dataframe['ID'] = dataframe['Index'].apply(lambda x: \"{}-{}\".format(doc_id,x))\n",
    "                dataframe['id'] = int(doc_id)\n",
    "                \n",
    "                \n",
    "                '''\n",
    "                for i in range(10):\n",
    "                    if not isinstance(dataframe['Parent Index'][i], int):\n",
    "                        dataframe['Parent Index'][i] = 0 # index是nan的補 0\n",
    "                '''\n",
    "                delta = 11\n",
    "                for i in range(0,len(dataframe),delta):\n",
    "                    sample = dataframe.loc[i:i+delta-1]\n",
    "                    groups.append(sample)\n",
    "                    \n",
    "                    text = ''.join(sample['Text']) \n",
    "                    if len(tokenizer.encode(text))>512:\n",
    "                        print(len(tokenizer.encode(text)), text)\n",
    "                    \n",
    "            return groups\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = get_samples(cinnamon_path)\n",
    "        self.tags = get_tags(cinnamon_path) if tags is None else tags\n",
    "\n",
    "        print(f'\\t[Info] Load Cannon_Dataset_Testing complete !! len:{self.__len__()}')    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "            \n",
    "    def collate_fn(self, samples):        \n",
    "        tokenizer, tags = self.tokenizer, self.tags\n",
    "            \n",
    "        CLS, SEP, PAD = tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id\n",
    "        \n",
    "        def zero_vec(): \n",
    "            return [0 for i in range(len(tags))]\n",
    "                \n",
    "        ## text tokenized, label vectoized\n",
    "        b_token_ids, b_token_indexs, b_doc_id = [], [], []\n",
    "        for sample in samples:\n",
    "            \n",
    "            doc_id = list(sample['doc_id'])[0]\n",
    "            \n",
    "            token_ids = [CLS]\n",
    "            token_indexs = [-1]\n",
    "            \n",
    "            for text, index in zip(sample['Text'], sample['Index']):\n",
    "                # 全形半形問題\n",
    "                text = str(text).replace('イ．','').replace('ア．','')\n",
    "                #tag = str(unicodedata.normalize(\"NFKC\", re.sub('＊|\\*|\\s+', '', tag))) if tag is not np.nan else tag\n",
    "                #value = str(value)\n",
    "                \n",
    "                ids = tokenizer.encode(text)[1:-1]# + [SEP]\n",
    "                \n",
    "                token_ids += ids\n",
    "                token_indexs += [index for jj in range(len(ids))]\n",
    "                \n",
    "            assert len(token_ids)==len(token_indexs)\n",
    "            b_token_ids.append(token_ids)\n",
    "            b_token_indexs.append(token_indexs)\n",
    "            b_doc_id.append(doc_id)\n",
    "\n",
    "        ## pad to same lenght\n",
    "        max_len = min([max([len(s) for s in b_token_ids]), 512])\n",
    "        for idx,(token_ids, token_indexs) in enumerate(zip(b_token_ids, b_token_indexs)):            \n",
    "            token_ids = token_ids[:max_len]\n",
    "            token_ids += [PAD]*(max_len-len(token_ids))\n",
    "            b_token_ids[idx] = token_ids\n",
    "            \n",
    "            token_indexs = token_indexs[:max_len]\n",
    "            token_indexs += [-1]*(max_len-len(token_indexs))\n",
    "            b_token_indexs[idx] = token_indexs\n",
    "\n",
    "        return torch.tensor(b_token_ids), None, b_token_indexs, b_doc_id, samples[0] #, torch.tensor(b_output)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Cinnamon_Dataset, DataLoader\n",
    "from train import * \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "\n",
    "valid_dataset = Cinnamon_Dataset_Testing('/media/D/ADL2020-SPRING/project/cinnamon/dev/', tokenizer, tags)\n",
    "valid_dataloader = DataLoader(valid_dataset,\n",
    "                             batch_size=1,\n",
    "                             collate_fn=valid_dataset.collate_fn,\n",
    "                             shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/train/', tokenizer)\n",
    "#tags = train_dataset.tags\n",
    "tags = ('仕様書交付期限',\n",
    "         '入札件名',\n",
    "         '入札書締切日時',\n",
    "         '入札書送付先',\n",
    "         '入札書送付先部署/担当者名',\n",
    "         '公告日',\n",
    "         '施設名',\n",
    "         '調達年度',\n",
    "         '調達終了日',\n",
    "         '調達開始日',\n",
    "         '資格申請締切日時',\n",
    "         '資格申請送付先',\n",
    "         '資格申請送付先部署/担当者名',\n",
    "         '質問票締切日時',\n",
    "         '質問箇所TEL/FAX',\n",
    "         '質問箇所所属/担当者',\n",
    "         '都道府県',\n",
    "         '開札場所',\n",
    "         '開札日時',\n",
    "         '需要場所(住所)')\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def fuller(text):\n",
    "    candidate = (ord('0'),ord('1'),ord('2'),ord('3'),ord('4'),ord('5'),ord('6'),ord('7'),ord('8'),ord('9'),\n",
    "            ord('('),ord(')'),ord('~'),)\n",
    "    text_out = ''\n",
    "    for c in text:\n",
    "        if ord(c) in candidate:\n",
    "            text_out += chr(ord(c)+65248)\n",
    "        else:\n",
    "            text_out += c\n",
    "    return text_out\n",
    "\n",
    "def post_process(value, tag, text):\n",
    "    '''\n",
    "    if tag=='質問箇所TEL/FAX':\n",
    "        value = value.replace('##l:','ＴＥＬ：').replace('tel:','ＴＥＬ：').replace('Tel:','ＴＥＬ：').replace('TEL:','ＴＥＬ：')\n",
    "        value = value.replace('fax:','ＦＡＸ：').replace('Fax:','ＦＡＸ：').replace('FAX:','ＦＡＸ：')\n",
    "    \n",
    "    print(text)\n",
    "    input(\"\")\n",
    "    \n",
    "    # 半形 轉 全形\n",
    "    value = fuller(value)\n",
    "    '''\n",
    "    value_ret = ''\n",
    "    for c in value:\n",
    "        if c in text:\n",
    "            value_ret += c\n",
    "        elif chr(ord(c)+65248) in text:\n",
    "            value_ret += chr(ord(c)+65248)\n",
    "        elif ord('a')<=ord(c) and ord(c)<=ord('z'): #小寫轉大寫\n",
    "            if chr(ord(c)-32) in text: #小寫轉大寫 半形\n",
    "                value_ret += chr(ord(c)-32)\n",
    "            elif chr(ord(c)+65248-32) in text: #小寫轉大寫 + 轉全形\n",
    "                value_ret += chr(ord(c)+65248-32)\n",
    "        elif ord('A')<=ord(c) and ord(c)<=ord('Z'): #大寫轉小寫\n",
    "            if chr(ord(c)+32) in text: #大寫轉小寫 半形\n",
    "                value_ret += chr(ord(c)+65248+32)            \n",
    "            elif chr(ord(c)+65248+32) in text: #大寫轉小寫 + 轉全形\n",
    "                value_ret += chr(ord(c)+65248+32)\n",
    "        else:\n",
    "            pass\n",
    "            #print(c, text, value)\n",
    "            \n",
    "    return value_ret\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "l = [0,1,2,2,2,1,9,\"a\",\"b\",\"b\"]\n",
    "Counter(l).most_common()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load('./ckpt/epoch_50.pt')['state_dict'])\n",
    "model.eval()\n",
    "\n",
    "total_dataframe = None \n",
    "for iii,(_input, _label, token_indexs, doc_id, sample) in enumerate(valid_dataloader):\n",
    "    sample['Prediction'] = \"\"\n",
    "    sample['Tag'] = \"\"\n",
    "    sample['Value'] = \"\"\n",
    "    \n",
    "    _output = model(_input)[0]\n",
    "    prob = F.sigmoid(_output)\n",
    "    \n",
    "    for i,tag in enumerate(tags):\n",
    "        index = [] #set()\n",
    "        values = []\n",
    "        for j in range(prob.size(0)):\n",
    "            if prob[j,i] > 0.5:\n",
    "                values.append(_input[0][j])\n",
    "                #index.update([token_indexs[0][j]])\n",
    "                index.append(token_indexs[0][j])\n",
    "                \n",
    "        if len(values)>0:\n",
    "            index = Counter(index).most_common()[0][0]\n",
    "            #print(index)\n",
    "            value_str = tokenizer.decode(values, skip_special_tokens=True).replace(\" \",\"\")            \n",
    "            value_str = post_process(value_str, tag, sample.loc[sample['Index']==index, 'Text'].item())\n",
    "            #value_str = post_process(value_str, tag)\n",
    "            \n",
    "            # add a tag&value to <Prediction>\n",
    "            '''\n",
    "            if sample[sample['Index']==index]['Prediction'].item() == \"\":\n",
    "                sample.loc[sample['Index']==index,'Prediction'] = \"{}: {} \".format(\n",
    "                            tag, value_str)\n",
    "            else :\n",
    "                sample.loc[sample['Index']==index,'Prediction'] += \"{}: {} \".format(\n",
    "                            tag, value_str)\n",
    "            '''\n",
    "            \n",
    "            # add a tag&value to <Tag> <Value>\n",
    "            if sample[sample['Index']==index]['Tag'].item() == \"\":\n",
    "                sample.loc[sample['Index']==index, 'Tag'] = \"{}\".format(tag)\n",
    "                sample.loc[sample['Index']==index, 'Value'] = \"{}\".format(value_str)\n",
    "            else:\n",
    "                sample.loc[sample['Index']==index, 'Tag'] += \";{}\".format(tag)\n",
    "                sample.loc[sample['Index']==index, 'Value'] += \";{}\".format(value_str)\n",
    "        \n",
    "    \n",
    "    total_dataframe = total_dataframe.append(sample) if isinstance(total_dataframe, pd.DataFrame) else sample\n",
    "\n",
    "    print(f'\\t[Info] [{iii+1}/{len(valid_dataloader)}]', end='   \\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_dataframe[total_dataframe['Tag']!=\"\"]\n",
    "total_dataframe = total_dataframe.sort_values(by=['id','Index'], ascending=[True,True])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_dataframe_clean = total_dataframe.drop('Page No', axis=1).drop('Parent Index', axis=1).drop('Is Title', axis=1).drop(\n",
    "                'Is Table', axis=1).drop('id', axis=1).drop('Index', axis=1)\n",
    "total_dataframe_clean.to_csv('./result/testout.csv', encoding='utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.convert import *\n",
    "from utils.score import * \n",
    "\n",
    "convert('./result/testout.csv','./result/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score('/media/D/ADL2020-SPRING/project/cinnamon/dev/dev_ref.csv','./result/submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9203583682805949"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_dataframe = total_dataframe.sort_values(by=['id','Index'], ascending=[True,True])\n",
    "for idx in total_dataframe.ID:\n",
    "    print(idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_dataframe.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('a'),ord('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('A'),ord('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a = set('令和2年5月22日(金)14時00分~'.split())\n",
    "b = set('令和２年５月２２日（金）１４時００分～'.split())\n",
    "len(a.intersection(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = tokenizer.encode('令和２年５月２２日（金）１４時００分～')\n",
    "tokenizer.decode(abc).replace(\" \",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "string = '令和２年５月２２日（金）１４時００分～'\n",
    "\n",
    "def _code(string):\n",
    "    coding = []\n",
    "    for char in string:\n",
    "        status = unicodedata.east_asian_width(char)\n",
    "        if status == 'F':\n",
    "            #print('{0} is full-width.'.format(char))\n",
    "            coding.append('F')\n",
    "        elif status == 'H':\n",
    "            #print('{0} is half-width.'.format(char))\n",
    "            coding.append('H')\n",
    "        else:\n",
    "            #print('{0} is char'.format(char))\n",
    "            coding.append('C')\n",
    "    return coding\n",
    "            \n",
    "_code(string)\n",
    "_code(\"１1１\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full = chr(ord('(')+65248)\n",
    "half = chr(ord(full)-65248)\n",
    "full, half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate = (ord('0'),ord('1'),ord('2'),ord('3'),ord('4'),ord('5'),ord('6'),ord('7'),ord('8'),ord('9'),\n",
    "            ord('('),ord(')'),ord('~'),)\n",
    "candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fuller(text):\n",
    "    candidate = (ord('0'),ord('1'),ord('2'),ord('3'),ord('4'),ord('5'),ord('6'),ord('7'),ord('8'),ord('9'),\n",
    "            ord('('),ord(')'),ord('~'),)\n",
    "    text_out = ''\n",
    "    for c in text:\n",
    "        if ord(c) in candidate:\n",
    "            text_out += chr(ord(c)+65248)\n",
    "        else:\n",
    "            text_out += c\n",
    "    return text_out\n",
    "\n",
    "text_out = fuller('令和2年5月22日(金)14時00分~')\n",
    "text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = set('調達年度:令和2年 調達終了日:令和5年3月31日 調達開始日:令和2年4月1日'.split())\n",
    "b = set('調達開始日:令和2年4月1日 調達終了日:令和5年3月31日 調達年度:令和2年'.split())\n",
    "len(a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(ord('3')+65248) in \"３ａｄｆａ１\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
